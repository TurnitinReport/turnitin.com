# turnitin.com
Turnitin Report
1.   INTRODUCTION
1.1 BACKGROUND
Botnets (DDoS agents) are networks of computers infected with a type of malware that allows individuals or organizations to take advantage of legitimate users to perform malicious actions such as denial of service attacks, spam, information theft and virtual fraud. These networks can contain thousands of bots, as enslaved computers are known, being a powerful tool under the control of criminals, with the potential to be used not only in cybercrimes but also in more alarming activities such as cyber terrorism and cyber war. In addition to the direct threat that botnets pose, the large volume of traffic generated consumes network and processing resources across the internet:
 
Figure 1.1: Structure of a botnet system [1]
1.2 PROBLEM STATEMENT
There are several techniques applicable to specific botnets, but the development of an effective technique and reliable system for detecting different types of botnets in a network is a challenge. As a difficulty, we can mention the constant appearance of new and more sophisticated botnet implementations, in order to circumvent all known defense techniques. A obvious proposal to address the problem is to use machine learning techniques to distinguish infected machines from healthy machines, however some problems need to be addressed such as: 
a.	Do bots have detectable patterns of behavior that allow their identification? 
b.	How to observe this behavior through computable data? 
c.	Machine Learning techniques demand large amounts of collected and classified data, are there databases of this type available? 
d.	Which machine learning algorithm would be best suited for the researched database? 
e.	What is the best way to implement a complete architecture that allows observation, identification and presentation of infected machines on a network?
1.3 MOTIVATION
Among the growing attacks, Botnets [1] have improved their ability to carry out attacks in a distributed manner, in evolved to impact large networks around the world, such as the invasion of financial systems to steal information from cards and bank passwords and in addition to stopping the operation of networks, through DDoS attacks.
 On mobile devices and their implemented services, this type of attack would cause great damage, due to the ease of infection and the ability to act during network intrusion.
 The use of defense against Botnet attacks, some tools are used such as:
a. Antivirus
b. Anti-spam
c. Firewalls, 
Among the current defense tools, the classification Systems stand out as an effective mechanisms that focus on the detection of intrusive activities in networks [4]. some of the classification technologies perform different from each other because of different types of environments therefore we implement multiple classification scheme in this thesis.
 
Figure 1.2: Popularity of botnets types used [3]
1.4 CONTRIBUTION
This thesis proposes the development of a system that classifies infected machines into botnets and non- botnets devices, the process of detecting botnets is done by using a machine learning algorithm to identify computers acting as bots in a network. This system that we propose should be able to read information from network elements, such as routers, and apply a classifier to identify the bot communities and present which machines are suspect. 
Different implementations of botnets have a common life cycle in common and there are specific stages of cycle that are vulnerable to detection, especially the connection phase. This specific pattern could be observed in the network traffic of an infected machine. 
In this thesis we use a reliable database, properly classified and in enough classes to obtain a classification model based on machine learning., there is a public database generated by network traffic flows from different devices simulated in a controlled environment with the presence of different known types of botnets. Thus, we can implement a system of botnet detection using the Support Vector Machine (SVM) and K-nearest neighbor KNN algorithm, which is currently one of the most used methods in the context of machine learning. SVM and KNN are a relatively easy to use algorithms, has a robust performance in pattern recognition, being considered the state of the art in pattern recognition.
1.5 THESIS ORGANIZATION
The thesis is structured as follows: Section 2 is where we review some of the previous work and implementation on botnets. Section 3 is where we give a background about all the components. Section 4 is where we explain our model in details and implementation of that model in details, Section 5 is where we simulate our model and record the results obtained. Section 6 is where we conclude our work and put a future scope into perspective.














 2.  LITERATURE REVIEW
In this chapter we review some of the related works done on botnet detection, [Tang et.al ] explained the growing importance of the security services such as botnet detection that have been provided over the Internet and how it is indisputable, increasing the dependence on the use of the network. As a result, data traffic grows exponentially [4] and, with this increase in use, there has also been an increase in the number of security incidents. [Hyunsang et.al] showed that among the malicious activities that generate these incidents are the sending of unauthorized e-mail (spam), theft of information, click fraud, which is the fraud of searches or other activities where the link click is counted, in addition to the dangerous Distributed Denial of Service (DDoS) attacks, which are denial of service (DoS) attacks carried out from distributed sources. The purpose of DDoS attacks is to stop a service by exhausting the victim's resources, so that the victim cannot respond to legitimate requests. In these malicious activities, so-called botnets are used, which are networks of infected machines (bots) and controlled by one or more attackers, called botmasters [5].
In [Chao Li et.al] 's work contributes in the context of a HIDS, identifying different C&C channels without checking the package' s payload, something that previous approaches could not. It evaluates the Decision Tree J48 and Random Forest algorithms in its own dataset, containing legitimate traffic and several bot families. The important hypotheses that botnet C&C traffic can be differentiated from others, including legitimate traffic, have been validated, and the characteristics of different styles of C&C are similar in different families of botnets. In [Schonewille et.al] an approach to extract C&C traffic signatures is presented using a dataset produced with the Anubis bot, examining the package payload. First, the most frequent “sequences” of the traffic are extracted, then a ranking function is used that will give a higher score to the “sequences” that appear frequently in one class of connections and rarely in others. This is due to the assumption that C&C connections from a malware family share similarities, while non-C&C connections are more diverse. With that, the authors obtained an accuracy superior to that of the other related works. In [Ramachandran et.al], flow-based attributes used in existing botnet detection studies are compared and evaluated. The approach used for this comparison was the Wrapper, with the decision tree classification algorithm A dataset was also created containing a diverse set of botnets and background traffic. In the work of [Wurzinger et.al], two machine learning algorithms, and SBB, are used to generate detection models for the Zeus bot. SBB was found to have the best performance. In [Binkley eta.l] a system is proposed to detect DDoS attacks from IP streams, a system that uses lambda architecture, which is the combination of batch processing and data processing (streaming processing). To validate the system, , Bagging and Adaptive Bagging algorithms were used, all based on Decision Tree, applied to two datasets: one with real data from the National Observatory network, mixed with simulated attack traffic, and one with three scenarios of the dataset. The system achieved excellent performance. In [Zhang et.al] a methodology for selecting attributes is proposed in order to detect botnets in the command-and-control phase. A genetic algorithm was used to select the set of attributes that provides the highest detection rate when using the algorithm for data classification. Experiments were carried out to extract the best attributes for each botnet analyzed and for each type of botnet in general. The work in [Schaffrath et.al performs an analysis of the packet payload, which today is not feasible since communication via C&C is usually encrypted. This work is validated in only one type of bot, greatly limiting the understanding of the general validity of this methodology. Regarding the evaluation of models to detect botnets, the comparison between the models studied in the works is not possible to be performed, as there is no standardization of datasets being investigated, as well as the sets of evaluation metrics used. Another existing factor is that, with the exception of the work in [Schoof et.al], the parameters used in the machine learning algorithms are not informed, making it impossible to reproduce the experiments. In addition, there is a scarcity of studies that broadly compare different classification algorithms involving the ML paradigms, for the purpose of detecting malicious events. On top of this problem, research was carried out in [Collins et.al] and [Davids et.al], which also used the classification dataset to raise attributes relevant to the detection of botnets through the attribute selection techniques. In these works, the efficiency of the J48, SVM, Naive Bayes and k-NN machine learning algorithms for detecting botnets traffic was analyzed. However, there is still a lack of supervised ML paradigms to be compared, which the research itself points to as future work. Among them, we can mention the connectionist paradigm, through the ANN algorithm. In addition, the work did not address unsupervised AM. Seeking to fill these gaps, the present work was developed, in which several paradigms of supervised and unsupervised ML are compared. In addition, a selection of characteristics is performed, comparing new attributes that were not calculated in [Brodsky et.al] and [Vrizlynn et.al]. Thus, it is intended to find out if there is an algorithm that satisfies the various scenarios of botnets studied or if a combination of these techniques should be used for a more effective classification, determining the combination of characteristics and ideal AM techniques for the problem of detecting botnets.



















 3.  MATERIALS AND METHODS
3.1 MACHINE LEARNING
In the Data Mining phase, Machine Learning algorithms, also known as Machine Learning, are used. It is an area of Artificial Intelligence that develops computational techniques on learning and builds systems with the ability to learn automatically and progressively, similar to human behavior. A machine learning system is capable of making decisions based on experiences obtained in solving previous problems. Learning has three components. One is representation, in which the learning system needs to find a set of models that perform well in generalizing data. If a model does not belong to the hypothesis set, it will not be used. Another component is the metric, obtained through an evaluation function, which will be used to discern the models that perform well in solving the problem. Finally, there is optimization. In this component, methods are used to find the best configuration for the parameters of each model. As mentioned in the introduction to this Dissertation, machine learning was considered the most effective technique for detecting botnets. Although this technique is quite sophisticated, there is no specific algorithm that performs best on all problems. Therefore, a methodology should be used to evaluate and compare the algorithms. These systems can be divided as to the mode, approach and form of learning used [9, 5] Induction is a reasoning to which a property extends to all terms in a set, that is, from the part to the whole. It starts with a specific concept and starts with general conclusions. Inductive inference is one of the main methods for extracting knowledge and predicting events, but it should be used with caution. If the number of examples is not sufficient or representative, the hypotheses may be worthless. Inductive learning is performed through examples external to the learning system, which can be supervised or unsupervised 
 
Figure 3.1: Types of machine learning [5]
In supervised learning, an induction algorithm is used that will carry out training in a set of instances labeled with the appropriate class. Each instance is composed of a vector of characteristic or attribute values and the class label. The goal is to build a classifier that can match the class of new unlabeled examples. If the value of the class is discrete, this learning is known as classification. Classification can be between two classes or multiclass, when there are more categories. If the class value is continuous, the learning is of the regression type [49]. For example, in this work, each instance used for training is labeled as malicious or normal. Therefore, the supervised learning algorithm looks for patterns in these labels through the selected characteristics, which can be the packet size, the duration or any other information of the network connection. After the algorithm is trained, the pattern found will be used to make predictions for test instances without labels. In unsupervised learning, on the other hand, the inductive algorithm analyzes the instances provided, not labeled, and tries to group them, if possible, in some way. With this, clusters or clusters are formed, which must be analyzed to understand the meaning of each one in the context of the problem studied [9]. For example, in this work, instances without a class label are provided to the unsupervised learning algorithm. Through the selected characteristics, this algorithm must group them into two clusters: normal and malicious. [5].
3.2 MACHINE LEARNING APPROACHS
The best known [5] are described in this subsection: 
a. Statistical Approach: The basis of this approach is to use statistical models to find a good approximation of the induced concept. These methods have parameters, so you can perform a parameterization, finding appropriate values for the model parameters from the data. Statistical methods that stand out are those of Bayesian learning. These methods use Bayes' theorem through a probabilistic model based on a priori knowledge of the problem. This model is combined with training examples to determine the final probability of a hypothesis. Examples of techniques that use this concept are Naive Bayes and Bayesian Networks. 
 
Figure 3.2: Statistical model vs learning model [9]
b. Symbolic Approach: In this approach, knowledge is built through a symbolic structure through the analysis of examples and counterexamples of a concept. These structures are representations of high-level knowledge and appear in the form of a logical expression, decision tree, rules or semantic network. 
c. Examples-Based Approach: Examples or instances never seen are classified through known examples. A new case will be classified through a similar case whose class is known. This learning is known as lazy, because it is necessary to keep the examples in memory to carry out new classifications. The opposite of this system is known as eager, since the examples are used to induce the model and are discarded after this process. The most important question is to know which examples are the most representative to be memorized by a lazy inductor in the training process. The k-nearest neighbors (k-NN) algorithm is one of the best known of this approach. 
 
Figure 3.3: ML padigrams [12]
d. Connectionist Approach: This area studies Neural Networks, which are mathematical constructions inspired by the biological model of the human nervous system. These models use interconnected processor units, neurons. This approach arouses the interest of researchers from different areas. The analogy with biology has indicated that neural networks have great potential to solve problems that require human sensory processing. Perceptual artificial neural networks with a layer or multilayer are examples of techniques that use this approach. 
e. Genetic or Evolutionary Approach: A derivation of the evolutionary model of learning, a genetic model is formed by a population of classification elements that compete with each other to perform the prediction. It begins with a population of elements where each represents a possible solution. These elements then compete with each other and those that perform less are discarded. The best ones are selected for reproduction, or crossover, and the new elements may or may not mutate. With that, the evolution occurs for several generations until the optimal solution to the problem is found. An analogy can be made with Darwin's theory of natural selection, where those better adapted to the environment have greater chances of survival than those less adapted, leaving a greater number of descendants. The best adapted organisms are selected for that environment. Genetic algorithms are examples of techniques of this approach.
3.3 MALICIOUS ACTIVITIES
Malware such as Botnets have the purpose of performing some malicious activity, activities that have a huge number of participants. Among the numerous malicious actions [3], the main activities are [6]:
a. Compromise of new hosts: to strengthen botnets, the botmaster can recruit new hosts using social engineering techniques, phishing and sending malicious emails.
b. DDoS: the DoS attack tries to prevent or reduce the legitimate use of a service, overloading the target by massively sending packets, impacting availability. In DDoS, on the other hand, multiple attackers act simultaneously to achieve this objective [2]. The botnet always has a set of mechanisms for flood execution such as SYN, ICMP and HTTP Flood or it can also send thousands of legitimate http and ftp requests [1], imitating the behavior of thousands of legitimate clients, making it extremely difficult to create schemes defense against a DDoS launched by botnets.
c. Spamming: Spam is unsolicited email created to be delivered to a large number of recipients [3]. The use of a botnet considerably increases the power to send a large amount of spam emails in a few seconds [5]. Botnets can use SMTP servers to send spam, most email spam today is sent via botnet [3].
d. Phishing: it is the creation of a replica of an existing web page or other online resource to deceive users, with the aim that they submit personal, financial or password data [4]. Bots can be used to host phishing sites [1]. To hide these replicas, hackers use the fast-flux technique [2].
e. Data theft: the botmaster can steal passwords and confidential information from users infected with bots. Various techniques can be used, such as screen capture, password theft, file uploading, keyloggers, cookie hijacking, etc.
f. Click baits: Online advertisements are charged for clicks on ads or number of visits to the site. This technique consists in deceiving users to induce them to click on ads or visit a website, increasing the revenue of third parties [5]. The use of botnets makes it possible to simulate the behavior of millions of legitimate users [6].
 
Figure 3.4: Botnet activities [6]
3.4 HOSTORY OF BOTNETS
Botnet is a network of compromised computers, called Bot, under the remote control of a human operator, the “Botmaster”. The term “Bot” is derived from the word “Robot.” Bots are host devices designed to perform some predefined functions in an automated way, which allows Botmaster to remotely control attack actions  other malware, such as viruses and worms, whose main focus is to attack the infected host, Botnets have a command and control structure, through C&C Servers (Commands and Control) that receive commands from the Botmaster and pass them on to the Bots, for that can execute the instructions received, using a distributed attack platform Botnet, as in some other services on the Internet, emerged as a useful tool. Bots were initially developed as a virtual individual, using a channel, to carry out activities of its owner, while it would be occupied elsewhere. The first Bot was developed in 1989, by Greg Lindahl, an operator server, in order to play the game Hunt the Wumpus with users. Ten years later, Pretty Park discovered the first worm that used an IRC server as a means of remote control. In the same year the SubSeven Trojan / Bot was launched, it was a remotely controlled Trojan horse, with resources for password theft [3]. In 2000, the Global Threat (GT) Bot was created, it was a Botnet that used IRC communication channels, using mIRC software, performed open port scans, distributed denial of service (DDoS) attacks, as well as access to a server control, but it did not have a direct spreading mechanism for its client Bots. In early 2002, Sdbot, written in C, was created by a Russian programmer known as SD. This Bot was more important for the evolution of Botnet technologies, as its source code was released by the author. Many started to use their concept or code, for the construction of future Bots, and in 2006, companies with Panda and Microsoft, reported variations of this malware [3]. In recent years, many Botnet technologies have been improved and have become more than attack tools, with a centralized server, for multiple servers with a decentralized structure.
3.5 BOTNET LIFE CYCLE
Botnets have a life cycle, with defined phases from the moment the victim is infected until they become part of the botnet and perform the malicious activities for which they are intended [5]. This cycle consists of the following phases: initial infection, secondary infection, connection, carrying out malicious activities and maintenance and updating, First, in the Initial Infection, the hacker infects the host by exploiting a vulnerability or the victim executes the malware, in this stage the machine becomes a potential bot. The second phase requires that the first has successfully completed, the infected computer runs a program that searches for binary codes of the malware in a repository, this download can be done via FTP, HTTP or P2P. When the download is finished and executed, this host starts to behave like a bot or zombie. The next phase is the connection or rally, in which the zombie machine comes into contact with the Command and Control (C&C) server, this occurs whenever it is restarted, where it informs that it is active in the botnet. In this stage, the Command and Control (C&C) channel is established, the bot then waits for the receipt of commands to execute the malicious activities In the last phase, procedures are carried out to preserve the botnet. This includes updating binary codes to add new functionality, evading detection or migrating from a C&C server In the phases of connection, carrying out malicious activities and maintenance and updating, the victim contacts the C&C servers. These phases are similar, regardless of the type of botnet, and are indicative that the malicious activity of a botnet, both in the control phase and in the attack phase itself, has behavioral patterns that can be systematically learned for detection purposes.
 
Figure 3.5: Botnet life cycle [7]
a.	INITIAL INFECTION
The Initial Infection phase, also known as Recruitment, is the first step in turning a vulnerable machine into a bot. New bot need to be recruited in order for the botmaster to have attack power. To do this, it is necessary to find as many vulnerable machines as possible. The behavior of the malware in this phase does not present special characteristics, it generally functions as a common Worm, having the ability to self-propagate. Scans are performed on the subnet to which the bot has access to detect known vulnerabilities, then various tactics are used to infect the machines. It is common to use exploits that can be downloaded on demand for each vulnerability found. Other methods of propagation can be used, such as sharing infected files.
b.	SECONDARY INJECTION
With success in the first phase, backdoors were installed allowing access to infected machines, but these are not yet considered bots, as they are not able to perform coordinated actions. In the second phase, called Secondary Injection, the infected machines run a script to obtain the malicious code from an online repository and install it. There must also be a botnet registration process. This process can be static or dynamic. In the static process, the information needed to join the botnet is present in the script, such as the IP of a command and control server. This type of process allows easy detection of C&C. In the dynamic process, this information must be obtained externally. One possible strategy is to use a file sharing network to provide the necessary data. After this phase, the machine effectively turns into a bot.
c.	CONNECTION
When the machine becomes a bot, it needs to connect to the C&C server to join the network and receive commands every time it is restarted. The way in which this step is carried out is one of the main concerns of botnet designers, as it is necessary to ensure that bots can contact the C&C infrastructure without exposing it. The DNS service is widely used to contact C&C in many well-known botnets. C&C infrastructures that use static IP addresses and are referenced directly by bots can be easily detected by reverse engineering malicious code. To make detection difficult, domain names are used, which periodically have the IP address modified with dynamic DNS techniques. Although the use of dynamic DNS makes it difficult to determine C&C addresses, it is still possible to identify malicious domains and block them. There are more sophisticated botnets that use a domain name generation technique called Domain Generation Algorithm (DGA), generating thousands of domain names periodically and using only a few to reference C&C. Another technique used to protect the integrity of C&C servers is to use static addresses only for intermediaries who provide lists of C&C servers available at that time (P2P). Different protocols are used in the communication, but the most common are IRC and HTTP in botnets with centralized architecture and P2P protocols in decentralized architecture. It is also possible to adopt the encryption of packages to make it difficult to detect malicious activities. Communication can take place in different directions depending on the implementation of Command and Control. One possible approach is for bots to request instructions periodically. This type of C&C is called PULL C&C, as in HTTP-based botnets. Another approach is that bots receive instructions from C&C passively, these are PUSH C&C, as in botnets that use IRC. This stage of the cycle can be considered vulnerable to detection, as bots need to communicate periodically with C&C. In addition to communicating with the C&C infrastructure, constant name resolutions to keep in touch with C&C (in botnets that use this technique) can also leave traces. These behavior patterns allow the development of botnet identification techniques.
3.6 BOTNET STRUCTURE
Over the years, network technology has become widely used in corporations, home environments and public networks, such as shopping centers, airports and restaurants, due to the numerous advantages presented by its use, providing humanity with breaking barriers, reducing distance, beyond the expansion of knowledge. The widespread deployment of networks brought new challenges to security and privacy, since its organizational structure often favors invasion by numerous forms of attack, such as denial of service, sniffing, among others. The vulnerability of computer networks due to attacks is a major problem throughout the business organization because new attacks are often discovered. Among the growing attacks, Botnets [1] have stood out for their ability to carry out attacks in a distributed manner and for the impacts caused on large networks around the world [1], such as the invasion of financial systems to steal information from cards and bank passwords and in addition to stopping the operation of networks, through DDoS attacks. On mobile devices and their implemented services, this type of attack would cause great damage, due to the ease of infection and the ability to act during network intrusion. Botnet is a network of infected computers controlled by one or more attackers to achieve certain objectives, which are the execution of malicious activities. Users whose devices are infected and are part of this network are generally unaware of the fact that they are indirectly participating in these malicious activities [19]. Botnets can be used for a variety of illicit activities such as Spam, DDoS attacks, distribution of malicious software (trojan, spyware and keylogger), software piracy, information theft, extortion, identity theft, game manipulation and online research, among others with the advent of IoT, the increase in the number of connected devices and vulnerabilities makes the environment conducive to the dissemination of these networks. 
These networks have enormous destructive potential, and can claim to be one of the biggest threats to the Internet.
 Bot is the malware installed on the victim's machine, which allows the execution of malicious actions, can be transmitted in different ways such as viruses, infected sites and the network.
Botmaster is the individual (hacker), organization or even a nation that has control over the botnet and can order the execution of malicious activities for these Bots. We call Botmaster the human controller of Botnet. It operates by controlling the Bots remotely, through commands sent to the C&C Server, which communicate with the Bots. Among these commands used, we can highlight some carried out on the IRC channels: 
a. C&C is the means by which botmasters send commands to bots, which can be a central server, for example using peer-to-peer (P2P) networks.
b. opencmd - opens the command prompt remotely.
c. recconect - causes Bot to reconnect to the channel.
d. repeat - repeats a given action a given number of times.
e. pingflood - ping an ip.
f. visit - makes Bot visit a specific url.
g. scan - performs a scan on an ip together with an established port.
h. udpflood - performs a udpflood on a given ip. 
i. execute - executes some process in visible or invisible mode.
Usually the Botnet is controlled by its creator, but many Botnets are created for commercialization, rented for criminal actions.
3.7 BOTNET TOPOLOGY
The main part of Botnet is a C&C server, responsible for the communication between Botmaster and Bots, through the forwarding of commands for the execution of actions. According to their command and control (C&C) structure, Botnets can be based on IRC, based on HTTP, or based on Peer to Peer (P2P) P2P botnets use the P2P protocol to avoid a single point of failure. In addition, P2P Botnes are more difficult to locate and shut down your C&C server. Most prevalent botnets are based on Internet Relay Chat (IRC) protocol , with centralized command and control mechanism. There are two topologies of Bots networks [8]: 
a. The centralized network based on the IRC or the HTTP protocol.  This approach is similar to the classic model of customer-server network. In this architecture, bots establish the communication channel with one or a few connection points which are the C&C servers. For example, the IRC protocol [22] can be used. Generally, C&C servers send commands to bots and provide malware updates. 
b. The Decentralized network based on P2P protocols. In this topology, a central point is responsible for the exchange of commands between Botmaster and Bot Clients. In this model, Botmaster chooses a host, usually a computer with broadband access, to be the central point, that is, the C&C Server, which monitors the status of the Bots and sends the instructions given by the owner. 
 
Figure 3.6: Types of Botnet topologies [22]
This C&C Server performs certain network services such as IRC or HTTP This topology is the most common, due to its simple structure, ease of management and high speed. The main advantage of this topology is the low latency of messages used by Botmaster to carry out attacks by the Bots, since all connections happen through the C&C Server. The disadvantage of this model is that the C&C Server is the critical point, if someone can discover its action, it can eliminate it and thus the entire Botnet will be useless and ineffective The Command and Control (C&C) channel is the botnet's backbone and can be classified according to their mode of operation and architecture: centralized, decentralized or hybrid. Decentralized modern botnets need flexibility and robustness to handle the large number of bots. They found in this decentralized architecture where communication is carried out in a distributed way based on protocols of P2P networks (Peer-to-Peer). This form of communication makes detection difficult and also provides resilience, since there is no longer a central point of failure [23]. 
 Hybrid: this approach combines the principles of Centralized and Decentralized architectures using hybrid P2P protocols together with the low network delay found in centralized architectures.
3.8 METHODS OF DETECTION
In given the potential of botnets, detection techniques are very important to curb this threat and have become an important research topic in recent years. The researchers developed several architectures and proposed a series of botnet detection taxonomies. Detection techniques are divided into two approaches. Based on honeynets, necessary to understand the technology and characteristics of the botnet, but not necessarily to detect the infection. Based on monitoring and analysis of network traffic through intrusion detection systems (IDS). The IDS can be classified as based on signature or anomaly, yet according to the audit source they can be classified as based on host or network [5]. All of these detection techniques will be described in this section:
3.9 DETECTION USING IDS
Security in most business environments today is based on the concept of defense in depth, where multiple layers of defenses are used to prevent adversaries from violating the company's security policies. Defense in depth is based on the premise that, even if an opponent penetrates one of the defense layers, he will not be able to do much damage, because the other layers will provide an adequate level of protection. Even though preventive mechanisms like Firewall, encryption and authentication, present the first line of defense for malicious users, an additional layer of defense called intrusion detection is often used to protect networks. Intrusion Detection Systems (IDS) are security tools that, as well as other measures, such as antivirus, Firewalls and access control systems, are intended to reinforce the security of information and communication systems [19]. IDS are considered the second security force, as it aims to collect and evaluate data from a system and to take preventive and protective measures [20]. In order to detect such behavior, intrusion detection systems typically contain two types of components [22]:
a. Data collection components;
b. Data analysis components.
 There are several advantages to implementing an Intrusion Detection System on a network:
a.	 The IDS acts as a deterrent for attackers. This IDS can also detect attacks that bypass other security measures. 
b.	it provides information about intrusions that can be used to respond to such attacks. 
c.	An IDS is also useful in carrying out quality control of network security and administration design.
Data collection components are made up of entities that are responsible for monitoring and collecting data on user and application activities. The collected data are then used by a second type of components, called component analysis Two main approaches to data collection have traditionally been used, which we classify into two types of intrusion detection systems [23]: 
a. Host-based IDS (Host Intrusion Detection System - HIDS) works on a Host and focuses on collection of your data, usually through operating system audit logs. 
b. Network-based IDS (Network Intrusion Detection System - NIDS) is network-based and focuses on data collection by monitoring traffic flowing through a network.
3.10 DETECTION USING HONEY NET OR HONEY ENCRYPTION
Honeypot is a system that acts as a decoy in order to attract Hackers' attention so that they attack it with the objective of protecting critical targets [38]. They are systems that have no production value, as they do not expect any data, any incoming and outgoing traffic is probably suspicious activity and should be investigated [31]. A honeynet is a network of Honeypots designed to be compromised and collect information from bots, however it has mechanisms so that it is not used to attack other networks After this collection, it is possible to understand the technology and analyze characteristics of the botnet, such as: bot signature for content-based detection, information about the C&C architecture and unknown vulnerabilities that allow bots to enter the network 
 
Figure 3.7: Basic concept of a honeynet [5]
Honeynets can also be used to obtain binaries from the bot and infiltrate the botnet. Honeynets have the following limitations [5]: 
a. They can monitor a limited number of activities.
b. They are unable to capture bots that do not use propagation methods or that use methods that require user interaction such as downloading and spam. 
c. They can only report on infected machines being used as a trap. 
d. With their popularity, hackers began to find ways to avoid these pitfalls for all these aspects, this technique is still very effective for collecting information from botnets but does not necessarily detect the infection.
3.11 DTECTION OF ABNORMAL ACTIVITY
Since there are many infected machines, neutralizing a single bot will not solve the problem. More effective methods seek to disable the entire malicious network [14]. These methods are based on combating the spread and communication of the botnet. Combating propagation reduces the number of compromised devices, limiting the power of the botnet. By stopping communication between infected devices and C&C servers, botmasters will be unable to send commands and receive responses from bots [5]. There are three main ways to defend against botnets: prevention, treatment and containment [8]. These measures must be taken together by users, network administrators and ISPs [5]. The techniques to prevent the spread of botnets aim to reduce the vulnerable population, limit the spread of worms and decrease the size of the botnet. As an example of these techniques, we have the secure development of software, systems with up-to-date security updates, removal of existing vulnerabilities, use of antivirus programs and user training [8]. The form of defense by treatment corresponds to the disinfection of infected devices to reduce the number of bots. The time required for new mechanisms and updates to be developed, in addition to the time for testing to verify the effectiveness of these methods in disinfecting zombie machines, is limited by the human time scale, which sometimes does not react quickly enough to combat malicious botnet activity and worm spread epidemics [4]. Containment has two phases: detection of botnets and response. In the detection phase, devices are monitored or the network is monitored - for example, through an IDS. In the response phase, mechanisms are used to block traffic between bots and C&C servers, with the ultimate goal of disabling these servers. Blocking can be automated after bot detection, without depending on human action. For this, automated systems integrated with firewalls, content filters, address blacklists and null routes can be used to block communications between infected devices and also to block the spread of malware. Thus, an infection can be stopped or reduced, the botnet communication can be stopped or the C&C center can be disabled [5]. These strategies can be implemented directly on the network without the need for a solution for each Internet host. In this work, the algorithms are located within the architecture of an IDS that will be monitoring a network in the detection phase. The tests were carried out with the intention of verifying, from the network traffic of an institution, activities originating from botnets located inside or outside the network of that institution.
3.12 SIGNATURE BASED DETECTION
This technique consists of extracting packet information from the monitored traffic and recording these patterns in an existing bot database. Apparently, it seems simple to perform the detections simply by comparing each byte in the packet, but this technique has serious disadvantages It is not possible to detect unknown bots (zero-day), as they have not been mapped. You should always update the knowledge base with new signatures, which reduces performance and increases management costs. New bots can perform malicious activities before the knowledge base is updated. In networks with high traffic, it will be difficult to process all packets. This technique does not allow encrypted traffic to be inspected.
3.13 DATAMINING
Data mining is the intermediate phase of the database knowledge discovery process, as seen in It is at this stage that useful information is extracted. Data mining techniques are designed to work in large databases in order to discover, through mathematical analysis, useful patterns that could be ignored or make predictions about future observation. These patterns could not be discovered with a traditional analysis due to the complexity of the relationships and the large amount of data, which is why methods involving machine learning, statistics and database systems are applied. 
 
Figure 3.8: Data mining pattern recognition [8]
The patterns that are extracted must then be transformed into an understandable structure for later use. The four central tasks of data mining are [7]: 
a. Cluster Analysis: The objective is to find and approximate related observations, so that those belonging to the same group are more similar to each other than those belonging to other groups. The name of these groupings is cluster. Examples of use are: recognizing patterns, grouping together related clients, detecting fraud, discovering areas of the ocean that have an impact on the climate and compacting data, among others.
 b. Predictive Modeling: The purpose is to build a model for a target variable as a function of explanatory variables. This model should minimize the error between the forecast and the actual value of the target variable. In this modeling there are two types of task: classification, in which the target variable must be discrete, and regression, in which the target variable must be continuous. Examples of use are: judging whether a patient has a certain disease, predicting whether a user will make a purchase and identifying whether a network connection is malicious are examples of classification, since the target variable is of binary value; predicting the future value of an action and estimating the amount to be spent on a project are examples of regression, since the target variable is of continuous value. 
c. Association Analysis: The purpose is to discover patterns that represent strongly associated characteristics. The discovered patterns are generally represented as rules of implication or subset of characteristics. The search space has an exponential size, so we seek to obtain the most important patterns efficiently. These rules can be represented as follows: if attribute A then attribute B. Examples of use are: products that are taken together in shopping baskets and discovery of web pages that are accessed together. 
d. Anomaly Detection: The objective is to identify observations that are quite different from others, these observations are called anomalies. The anomaly detection algorithm must discover the true anomalies, looking for a high rate of true positives and a low rate of false positives. Examples of use are: disturbances in the environment and detection of unusual diseases.
3.14 KDD DATASET
Also known as KDD, from the words Knowledge Discovery in Databases, it is the process of converting raw data, present in large databases, to useful information. This process has several stages of data transformation, starting in pre-processing and ending in post-processing. In data entry, the most diverse file formats can be used, such as plain text, spreadsheets and relational tables. This data can be centralized or distributed in numerous repositories. The objective of the pre-processing phase is to transform the raw data into the format that will be used in the process, so the data is grouped and organized according to the problem domain. In this first phase, data can be merged from different repositories, calculations of new attributes from the original data, as well as cleaning the data to remove noise, inconsistencies and redundant observations. In this phase, a selection of characteristics relevant to the task is usually carried out, where information not necessary for the problem domain is removed and only relevant characteristics are manipulated in the database. In addition, the data is formatted so that it can be used in the selected techniques and algorithms. 
 
Figure 3.9: KDD dataset features [7]
This is the most laborious and time-consuming phase, as much attention is needed to select the relevant data [7]. In the last phase, called post-processing, only useful and valid results for the problem are selected, results that generate knowledge for user interpretation, evaluation and validation.

